{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2508,"status":"ok","timestamp":1730213842291,"user":{"displayName":"Tikhon Borkowski","userId":"04129612924906965167"},"user_tz":-240},"id":"nFqp270uNVWR","outputId":"3a35a4fa-48ab-4c6c-ad53-5aaf281a44f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import os\n","import requests\n","from bs4 import BeautifulSoup\n","import tarfile\n","import pandas as pd\n","from tqdm import tqdm\n","from concurrent.futures import ThreadPoolExecutor\n","import logging\n","from google.colab import drive\n","\n","# Монтируем Google Drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bzlPO_-pNUVh"},"outputs":[],"source":["import os\n","import logging\n","\n","# Настройки логирования\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","\n","# Основные параметры и настройки\n","base_urls = [\n","    \"https://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_package/00/\",\n","    \"https://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_package/01/\",\n","    \"https://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_package/02/\",\n","    \"https://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_package/03/\",\n","    \"https://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_package/04/\",\n","    \"https://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_package/05/\",\n","    \"https://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_package/06/\",\n","    \"https://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_package/07/\",\n","    \"https://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_package/08/\",\n","    \"https://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_package/09/\",\n","]\n","\n","# Директории для временных файлов в сессии\n","save_directory = \"/content/pmc_files\"\n","extracted_directory = \"/content/extracted_xml_files\"\n","\n","# Директория для готовых CSV-файлов на Google Диске\n","csv_output_dir = \"/content/drive/MyDrive/Colab Notebooks/pmc_articles/raw_population_data/csv_files\"  # Директория для сохранения CSV-файлов\n","\n","max_files = 20000\n","num_threads = 10\n","\n","# Создание директорий, если они не существуют\n","os.makedirs(save_directory, exist_ok=True)\n","os.makedirs(extracted_directory, exist_ok=True)\n","os.makedirs(csv_output_dir, exist_ok=True)\n","\n","# Логирование для подтверждения создания директорий\n","logging.info(f\"Директория для сохранения файлов: {save_directory}\")\n","logging.info(f\"Директория для извлеченных XML файлов: {extracted_directory}\")\n","logging.info(f\"Директория для готовых CSV файлов: {csv_output_dir}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J6e9OmTMdRnm"},"outputs":[],"source":["# Функция для получения списка файлов из директории на сервере по указанному URL\n","def get_files_from_directory(directory_url):\n","    response = requests.get(directory_url)\n","    soup = BeautifulSoup(response.text, \"html.parser\")\n","    return [link.get(\"href\") for link in soup.find_all(\"a\") if link.get(\"href\").endswith(\".tar.gz\")]\n","\n","# Функция для загрузки одного файла по указанному URL\n","def download_file(file_url, file_number):\n","    try:\n","        logging.info(f\"Начинается загрузка файла #{file_number}: {file_url}\")\n","        response = requests.get(file_url, stream=True)\n","        response.raise_for_status()\n","        filename = os.path.join(save_directory, os.path.basename(file_url))\n","        with open(filename, \"wb\") as file:\n","            for chunk in response.iter_content(chunk_size=8192):\n","                file.write(chunk)\n","        logging.info(f\"Загружен: {filename}\")\n","    except requests.exceptions.RequestException as e:\n","        logging.error(f\"Ошибка загрузки {file_url}: {e}\")\n","\n","# Функция для загрузки всех файлов из поддиректорий на сервере с использованием многопоточности\n","def download_all_files(base_url, max_files):\n","    all_files = []\n","    response = requests.get(base_url)\n","    soup = BeautifulSoup(response.text, \"html.parser\")\n","    subdirectories = [link.get(\"href\") for link in soup.find_all(\"a\") if link.get(\"href\").endswith(\"/\")]\n","\n","    for subdir in subdirectories:\n","        subdir_url = f\"{base_url}{subdir}\"\n","        files = get_files_from_directory(subdir_url)\n","        all_files.extend([f\"{subdir_url}{file}\" for file in files])\n","        if len(all_files) >= max_files:\n","            break\n","\n","    total_files = len(all_files[:max_files])\n","    logging.info(f\"Всего файлов для загрузки: {total_files} из {base_url}\")\n","\n","    # Загрузка файлов с использованием потоков\n","    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n","        list(tqdm(executor.map(lambda x: download_file(x[1], x[0]), enumerate(all_files[:max_files])), total=total_files, desc=\"Загрузка\", unit=\"file\", ncols=100))\n","\n","# Функция для распаковки файлов tar.gz\n","def extract_xml_files(tar_file):\n","    try:\n","        with tarfile.open(tar_file, \"r:gz\") as tar:\n","            for member in tar.getmembers():\n","                if member.name.endswith('.nxml') or member.name.endswith('.xml'):\n","                    member.name = os.path.basename(member.name)\n","                    tar.extract(member, extracted_directory)\n","                    logging.info(f\"Распакован: {member.name} из {tar_file}\")\n","    except (tarfile.ReadError, EOFError) as e:\n","        logging.error(f\"Ошибка распаковки {tar_file}: {e}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_3Y_VpdENcJR"},"outputs":[],"source":["# Функция для обработки XML файлов и сохранения информации в CSV\n","def process_xml_files(directory, output_csv):\n","    all_data = []  # Список для хранения информации по всем статьям\n","\n","    # Получаем список всех XML файлов в директории\n","    xml_files = [f for f in os.listdir(directory) if f.endswith('.nxml') or f.endswith('.xml')]\n","\n","    for file_name in tqdm(xml_files, desc=\"Обработка\", unit=\"file\", ncols=100):\n","        file_path = os.path.join(directory, file_name)  # Полный путь к файлу\n","        with open(file_path, 'r', encoding='utf-8') as file:\n","            soup = BeautifulSoup(file.read(), 'lxml')  # Парсим XML файл с помощью BeautifulSoup\n","\n","            # Извлечение названия статьи\n","            title = soup.find('article-title').get_text(strip=True) if soup.find('article-title') else None\n","\n","            # Извлечение авторов\n","            authors = []\n","            for contrib in soup.find_all('contrib', {'contrib-type': 'author'}):\n","                surname = contrib.find('surname').get_text(strip=True) if contrib.find('surname') else ''\n","                given_names = contrib.find('given-names').get_text(strip=True) if contrib.find('given-names') else ''\n","                authors.append(f\"{given_names} {surname}\".strip())  # Формируем полное имя автора\n","            authors = ', '.join(authors)  # Объединяем имена авторов через запятую\n","\n","            # Извлечение ссылки на статью PMC\n","            pmc_id = soup.find('article-id', {'pub-id-type': 'pmc'})\n","            pmc_link = f\"https://pmc.ncbi.nlm.nih.gov/articles/PMC{pmc_id.get_text(strip=True)}/\" if pmc_id else None\n","\n","            # Извлечение основного текста статьи\n","            body = soup.find('body')\n","            full_text = ''\n","            if body:\n","                # Ищем все секции <sec> внутри <body>\n","                sections = body.find_all('sec')\n","                for sec in sections:\n","                    # Объединяем текст всех <p> внутри текущей секции\n","                    paragraphs = sec.find_all('p', recursive=False)\n","                    sec_text = ' '.join(para.get_text(strip=True) for para in paragraphs if para.get_text(strip=True))\n","\n","                    if sec_text:  # Проверяем, что текст не пустой\n","                        full_text += sec_text + '\\n\\n'  # Добавляем пробел между абзацами\n","\n","            # Добавление извлеченной информации в общий список, если текст не пустой\n","            if title and authors and pmc_link and full_text.strip():\n","                all_data.append({\n","                    'Title': title,\n","                    'Authors': authors,\n","                    'PMC Link': pmc_link,\n","                    'Text': full_text.strip()  # Обрезаем лишние пробелы\n","                })\n","\n","    # Сохранение всех данных в CSV файл\n","    pd.DataFrame(all_data).to_csv(output_csv, index=False)\n","    logging.info(f\"Сохранен CSV файл: {output_csv}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"qGkmN0S3NydT","outputId":"40d9241d-572a-46ec-9bbf-94642f56cfae"},"outputs":[{"name":"stderr","output_type":"stream","text":["Загрузка: 100%|█████████████████████████████████████████████| 20000/20000 [21:52<00:00, 15.23file/s]\n","Распаковка: 100%|███████████████████████████████████████████| 20000/20000 [25:54<00:00, 12.87file/s]\n","Обработка:   8%|███▋                                         | 1524/18673 [02:43<33:12,  8.61file/s]<ipython-input-9-75b1c4b4a697>:11: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n","  soup = BeautifulSoup(file.read(), 'lxml')  # Парсим XML файл с помощью BeautifulSoup\n","Обработка: 100%|████████████████████████████████████████████| 18673/18673 [33:45<00:00,  9.22file/s]\n","Загрузка: 100%|█████████████████████████████████████████████| 20000/20000 [23:58<00:00, 13.90file/s]\n","Распаковка: 100%|███████████████████████████████████████████| 20000/20000 [26:04<00:00, 12.78file/s]\n","Обработка: 100%|████████████████████████████████████████████| 18764/18764 [34:04<00:00,  9.18file/s]\n","Загрузка: 100%|█████████████████████████████████████████████| 20000/20000 [25:43<00:00, 12.96file/s]\n","Распаковка: 100%|███████████████████████████████████████████| 20000/20000 [31:40<00:00, 10.53file/s]\n","Обработка: 100%|████████████████████████████████████████████| 18795/18795 [35:25<00:00,  8.84file/s]\n","Загрузка: 100%|█████████████████████████████████████████████| 20000/20000 [23:13<00:00, 14.35file/s]\n","Распаковка: 100%|███████████████████████████████████████████| 20000/20000 [30:10<00:00, 11.04file/s]\n","Обработка: 100%|████████████████████████████████████████████| 18669/18669 [35:28<00:00,  8.77file/s]\n","Загрузка: 100%|█████████████████████████████████████████████| 20000/20000 [23:47<00:00, 14.01file/s]\n","Распаковка: 100%|███████████████████████████████████████████| 20000/20000 [27:52<00:00, 11.96file/s]\n","Обработка: 100%|████████████████████████████████████████████| 18767/18767 [35:13<00:00,  8.88file/s]\n","Загрузка: 100%|█████████████████████████████████████████████| 20000/20000 [31:36<00:00, 10.55file/s]\n","Распаковка: 100%|███████████████████████████████████████████| 20000/20000 [27:45<00:00, 12.01file/s]\n","Обработка: 100%|████████████████████████████████████████████| 18698/18698 [35:27<00:00,  8.79file/s]\n","Загрузка: 100%|█████████████████████████████████████████████| 20000/20000 [25:10<00:00, 13.24file/s]\n","Распаковка: 100%|███████████████████████████████████████████| 20000/20000 [27:09<00:00, 12.27file/s]\n","Обработка: 100%|████████████████████████████████████████████| 18828/18828 [35:31<00:00,  8.83file/s]\n","Загрузка:  39%|█████████████████▊                            | 7728/20000 [08:11<47:30,  4.31file/s]ERROR:root:Ошибка загрузки https://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_package/07/4f/PMC9736897.tar.gz: ('Connection broken: IncompleteRead(377372672 bytes read, 249656771 more expected)', IncompleteRead(377372672 bytes read, 249656771 more expected))\n","Загрузка: 100%|█████████████████████████████████████████████| 20000/20000 [23:16<00:00, 14.32file/s]\n","Распаковка:  26%|███████████▌                                | 5283/20000 [06:06<17:11, 14.27file/s]ERROR:root:Ошибка распаковки /content/pmc_files/PMC9736897.tar.gz: Compressed file ended before the end-of-stream marker was reached\n","Распаковка: 100%|███████████████████████████████████████████| 20000/20000 [26:09<00:00, 12.74file/s]\n","Обработка: 100%|████████████████████████████████████████████| 18929/18929 [34:18<00:00,  9.19file/s]\n","Загрузка: 100%|█████████████████████████████████████████████| 20000/20000 [24:36<00:00, 13.55file/s]\n","Распаковка: 100%|███████████████████████████████████████████| 20000/20000 [28:00<00:00, 11.90file/s]\n","Обработка: 100%|████████████████████████████████████████████| 18768/18768 [33:44<00:00,  9.27file/s]\n","Загрузка: 100%|█████████████████████████████████████████████| 20000/20000 [26:53<00:00, 12.40file/s]\n","Распаковка: 100%|███████████████████████████████████████████| 20000/20000 [27:50<00:00, 11.97file/s]\n","Обработка: 100%|████████████████████████████████████████████| 18895/18895 [33:50<00:00,  9.30file/s]\n"]}],"source":["# Основной цикл обработки каждого base_url\n","csv_counter = 0\n","for base_url in base_urls:\n","    download_all_files(base_url, max_files)\n","\n","    # Распаковка всех загруженных tar.gz файлов\n","    tar_files = [f for f in os.listdir(save_directory) if f.endswith('.tar.gz')]\n","    for tar_file in tqdm(tar_files, desc=\"Распаковка\", unit=\"file\", ncols=100):\n","        extract_xml_files(os.path.join(save_directory, tar_file))\n","\n","    # Удаление tar.gz файлов после распаковки\n","    for tar_file in tar_files:\n","        try:\n","            os.remove(os.path.join(save_directory, tar_file))\n","            logging.info(f\"Удалён файл: {tar_file}\")\n","        except OSError as e:\n","            logging.error(f\"Ошибка удаления {tar_file}: {e}\")\n","\n","    # Определение имени выходного CSV файла\n","    output_csv = f\"{csv_output_dir}/cleaned_articles_{csv_counter:02d}.csv\"\n","    process_xml_files(extracted_directory, output_csv)\n","\n","    # Удаление распакованных XML файлов после обработки\n","    xml_files = [f for f in os.listdir(extracted_directory) if f.endswith('.nxml') or f.endswith('.xml')]\n","    for xml_file in xml_files:\n","        try:\n","            os.remove(os.path.join(extracted_directory, xml_file))\n","            logging.info(f\"Удалён файл: {xml_file}\")\n","        except OSError as e:\n","            logging.error(f\"Ошибка удаления {xml_file}: {e}\")\n","\n","    csv_counter += 1"]}],"metadata":{"colab":{"provenance":[{"file_id":"1I1hGKQRu_1_su9yjesmuoxVm9vybLqZQ","timestamp":1730291435663},{"file_id":"1l1xIjJrvWxemkb5hVXIJAcjQZ5ZEsHdw","timestamp":1730210613332},{"file_id":"1jLpkgobml04Tf2WIsDr_6Bhl2B27jpyD","timestamp":1729086190236}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}